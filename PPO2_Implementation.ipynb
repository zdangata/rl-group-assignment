{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSEUDO CODE:\n",
    "\n",
    "initialize_policy_network()\n",
    "initialize_value_network()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = policy_network.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Store the experience tuple (s, a, r, s', done)\n",
    "        store_experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        episode_rewards += reward\n",
    "        state = next_state\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        minibatch = sample_minibatch_from_experience_buffer()\n",
    "        \n",
    "        for transition in minibatch:\n",
    "            calculate_advantage_estimate(transition)\n",
    "            calculate_value_estimate(transition)\n",
    "            calculate_policy_loss(transition)\n",
    "            calculate_value_loss(transition)\n",
    "            calculate_entropy_bonus(transition)\n",
    "            calculate_total_loss()\n",
    "            update_policy_and_value_networks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "#Do we want to use tensor board? \n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
