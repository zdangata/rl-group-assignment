{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # for interacting with environment\n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    # for ppo update\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        # action_logprobs indirectly represents the policy $\\pi_{\\theta}(s,a)$\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "class PPO():\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.ent_coef = 0.01\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory, timestep, total_timestep):   \n",
    "        # Monte Carlo estimate of state rewards (can be replaced by General Advantage Estimators)\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # learning rate annealing\n",
    "            frac = (timestep - 1.0) / total_timestep\n",
    "            new_lr = self.lr * (1.0 - frac)\n",
    "            new_lr = max(new_lr, 0.0)\n",
    "            self.optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "\n",
    "            # entropy decay\n",
    "            self.ent_coef = max(0.001, self.ent_coef * (1.0 - frac))\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            \n",
    "            # Finding Surrogate Loss (no gradient in advantages)\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            # MseLoss is for the update of critic, dist_entropy denotes an entropy bonus\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - self.ent_coef*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)  # Gradient clipping\n",
    "            self.optimizer.step()\n",
    "\n",
    "    \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.003 Adam betas: (0.9, 0.999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lijun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 \t avg length: 117 \t reward: -200\n",
      "Episode 40 \t avg length: 113 \t reward: -186\n",
      "Episode 60 \t avg length: 112 \t reward: -157\n",
      "Episode 80 \t avg length: 116 \t reward: -158\n",
      "Episode 100 \t avg length: 116 \t reward: -188\n",
      "Episode 120 \t avg length: 108 \t reward: -134\n",
      "Episode 140 \t avg length: 106 \t reward: -109\n",
      "Episode 160 \t avg length: 100 \t reward: -77\n",
      "Episode 180 \t avg length: 109 \t reward: -91\n",
      "Episode 200 \t avg length: 113 \t reward: -138\n",
      "Episode 220 \t avg length: 109 \t reward: -141\n",
      "Episode 240 \t avg length: 103 \t reward: -89\n",
      "Episode 260 \t avg length: 108 \t reward: -92\n",
      "Episode 280 \t avg length: 104 \t reward: -70\n",
      "Episode 300 \t avg length: 109 \t reward: -60\n",
      "Episode 320 \t avg length: 113 \t reward: -53\n",
      "Episode 340 \t avg length: 155 \t reward: -49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Running policy_old:\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# also append state, action, action_logprobs to the memory\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 49\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_old\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Saving reward and is_terminal:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[1;34m(self, state, memory)\u001b[0m\n\u001b[0;32m     56\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_layer(state)\n\u001b[0;32m     57\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[1;32m---> 58\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m memory\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     61\u001b[0m memory\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[1;32mc:\\Users\\lijun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[0;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "############## Hyperparameters ##############\n",
    "env_name = \"LunarLander-v2\"\n",
    "# creating environment\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env = RecordVideo(env=env, video_folder=\"./videos\", name_prefix=\"test-video\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "render = False\n",
    "solved_reward = 230         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 1500        # max training episodes\n",
    "max_timesteps = 1000         # max timesteps in one episode\n",
    "n_latent_var = 256           # number of variables in hidden layer\n",
    "update_timestep = 2000      # update policy every n timesteps; batch timesteps\n",
    "lr = 0.003\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 8                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = 42\n",
    "#############################################\n",
    "\n",
    "# if random_seed:\n",
    "#     torch.manual_seed(random_seed)\n",
    "#     env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print('learning rate:',lr, 'Adam betas:', betas)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0\n",
    "\n",
    "rew_for_plots = 0\n",
    "rew_for_plots_list = []\n",
    "total_timestep = max_episodes * max_timesteps\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset(seed=random_seed)[0]\n",
    "    if i_episode % 200 == 0:\n",
    "        env.start_video_recorder()\n",
    "    for t in range(max_timesteps):\n",
    "        timestep += 1\n",
    "        \n",
    "        # Running policy_old:\n",
    "        # also append state, action, action_logprobs to the memory\n",
    "        with torch.no_grad():\n",
    "            action = ppo.policy_old.act(state, memory)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        # Saving reward and is_terminal:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            ppo.update(memory, timestep, total_timestep)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "            break\n",
    "        \n",
    "        running_reward += reward\n",
    "        rew_for_plots += reward\n",
    "\n",
    "        # if render:\n",
    "        #     env.render()\n",
    "        if done or truncated:\n",
    "            break\n",
    "    if i_episode % 200 == 0:\n",
    "        env.close_video_recorder()\n",
    "    avg_length += t\n",
    "    rew_for_plots_list.append(rew_for_plots)\n",
    "    rew_for_plots = 0\n",
    "\n",
    "    \n",
    "    # stop training if avg_reward > solved_reward\n",
    "    # if running_reward > (log_interval*solved_reward):\n",
    "    #     print(\"########## Solved! ##########\")\n",
    "    #     break\n",
    "\n",
    "    if timestep >= total_timestep:\n",
    "        break\n",
    "    \n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        avg_length = int(avg_length/log_interval)\n",
    "        running_reward = int((running_reward/log_interval))\n",
    "        # print(rew_for_plots)\n",
    "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# def running_mean(x):\n",
    "#     N=50\n",
    "#     kernel = np.ones(N)\n",
    "#     conv_len = x.shape[0]-N\n",
    "#     y = np.zeros(conv_len)\n",
    "#     for i in range(conv_len):\n",
    "#         y[i] = kernel @ x[i:i+N]\n",
    "#         y[i] /= N\n",
    "#     return y\n",
    "\n",
    "\n",
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N) / N\n",
    "    return np.convolve(x, kernel, mode='same')\n",
    "\n",
    "rew_for_plots_list = np.array(rew_for_plots_list)\n",
    "avg_score = running_mean(rew_for_plots_list)\n",
    "# episode_rewards_history_rand = np.array(episode_rewards_history_rand)  # random agent\n",
    "# avg_score_rand = running_mean(episode_rewards_history_rand)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.ylabel(\"Rewards\",fontsize=12)\n",
    "plt.xlabel(\"Episodes\",fontsize=12)\n",
    "plt.plot(rew_for_plots_list, color='gray' , linewidth=1)\n",
    "plt.plot(avg_score, color='blue', linewidth=3,label = 'Running Average Score of PPO Policy')\n",
    "# plt.plot(avg_score_rand, color='orange', linewidth=3,label = 'Running Average Score of Random Policy')\n",
    "plt.axhline(y=200, color='r', linestyle='-',label = 'Solved')\n",
    "plt.scatter(np.arange(rew_for_plots_list.shape[0]),rew_for_plots_list, \n",
    "            color='green' , linewidth=0.3, label='Episode Rewards')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Rewards by Episode For PPO in Lunar Lander')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Serializing the list\n",
    "with open('list_name.pkl', 'wb') as f:\n",
    "    pickle.dump(rew_for_plots_list, f)\n",
    "\n",
    "# Deserializing the list\n",
    "with open('list_name.pkl', 'rb') as f:\n",
    "    loaded_list = pickle.load(f)\n",
    "\n",
    "print(loaded_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
