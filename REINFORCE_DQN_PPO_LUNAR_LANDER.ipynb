{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisweekes/anaconda3/envs/myenv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m policy_net, episode_rewards_history\n\u001b[1;32m     34\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m,render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m random_l, episode_rewards_history_rand \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_lunar\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msay \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mrandom_lunar\u001b[0;34m(env, num_episodes, max_steps_per_episode)\u001b[0m\n\u001b[1;32m     29\u001b[0m         state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     31\u001b[0m     episode_rewards_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(episode_rewards))\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy_net\u001b[49m, episode_rewards_history\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "#Random Policy\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "import random \n",
    "\n",
    "def random_lunar(env, num_episodes, max_steps_per_episode):\n",
    "\n",
    "    episode_rewards_history = []\n",
    "    #last_episode_video_path = './Documents/Reinforcement_Learning/RL_proj/rl-group-assignment'\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_rewards = []\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            num_actions = env.action_space.n\n",
    "            action = random.randint(0, num_actions-1)\n",
    "            next_state, reward, done,_, _ = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "        episode_rewards_history.append(sum(episode_rewards))\n",
    "    return episode_rewards_history\n",
    "\n",
    "env = gym.make('LunarLander-v2',render_mode = \"rgb_array\")\n",
    "episode_rewards_history_rand = random_lunar(env, num_episodes=2000, max_steps_per_episode=1000)\n",
    "\n",
    "os.system('say \"Finished\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REINFORCE \n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "#from gym.wrappers.record_video import RecordVideo\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Reinforce algorithm\n",
    "def reinforce(env, num_episodes, max_steps_per_episode, gamma=0.99, lr=0.001):\n",
    "    policy_net = PolicyNetwork(state_size=env.observation_space.shape[0],\n",
    "                               action_size=env.action_space.n)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    episode_rewards_history = []\n",
    "    #last_episode_video_path = './Documents/Reinforcement_Learning/RL_proj/rl-group-assignment'\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_rewards = []\n",
    "        log_probs = []\n",
    "\n",
    "        #if episode == num_episodes-1:\n",
    "         #   env.start_video_recorder()\n",
    "        #    pass\n",
    "        \n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "\n",
    "\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = policy_net(state_tensor)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "            #print(env.step(action.item()))\n",
    "            next_state, reward, done,_, _ = env.step(action.item())\n",
    "            episode_rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "        #if episode == num_episodes-1:\n",
    "        #    pass\n",
    "         ##   env.close_video_recorder()\n",
    "\n",
    "        # Compute the discounted cumulative reward\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for r in episode_rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "\n",
    "        episode_rewards_history.append(sum(episode_rewards))\n",
    "        \n",
    "        # Normalize the discounted rewards\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Compute the policy gradient and update the policy network\n",
    "        policy_losses = []\n",
    "        for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "            policy_losses.append(-log_prob * reward)\n",
    "        \n",
    "        policy_loss = torch.stack(policy_losses).sum()\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "    #env.close()   \n",
    "    return policy_net, episode_rewards_history\n",
    "\n",
    "# Train the policy network\n",
    "env = gym.make('LunarLander-v2',render_mode = \"rgb_array\")\n",
    "#env = RecordVideo(env = env, video_folder=\"./videos\",name_prefix='Last_episode_video')\n",
    "\n",
    "policy_net, episode_rewards_history_reinforce = reinforce(env, num_episodes=2000, max_steps_per_episode=1000)\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time)\n",
    "\n",
    "def running_mean(x):\n",
    "    N=50\n",
    "    kernel = np.ones(N)\n",
    "    conv_len = x.shape[0]-N\n",
    "    y = np.zeros(conv_len)\n",
    "    for i in range(conv_len):\n",
    "        y[i] = kernel @ x[i:i+N]\n",
    "        y[i] /= N\n",
    "    return y\n",
    "\n",
    "episode_rewards_history_reinforce = np.array(episode_rewards_history_reinforce)\n",
    "reinforce_mean_rewards = running_mean(episode_rewards_history_reinforce)\n",
    "\n",
    "os.system('say \"Finished\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#%% Policy network\n",
    "class QNet(nn.Module):\n",
    "    # Policy Network\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        #print('state1: ', state)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        #print('state2: ', state)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "def running_mean(iterations, score_hist):\n",
    "    window_size = 100 #for the running mean\n",
    "\n",
    "    if iterations == 0:\n",
    "        score_avg = score_hist[0]\n",
    "    elif iterations < window_size:\n",
    "        score_avg = np.mean(score_hist[:iterations])\n",
    "    else:\n",
    "        score_avg = np.mean(score_hist[iterations - window_size : iterations])\n",
    "\n",
    "\n",
    "    return score_avg\n",
    "\n",
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.1, eps_decay=0.995, target=200, chkpt=False):\n",
    "    score_hist = []\n",
    "    mean_rewards = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    # bar_format = '{l_bar}{bar:10}{r_bar}'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    for idx_epi in pbar:\n",
    "        state = env.reset()[0]\n",
    "        score = 0\n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            movement = env.step(action)\n",
    "            next_state, reward, done, _ = movement[0], movement[1], movement[2], movement[4]\n",
    "            #reward = np.clip(reward, -1, 1)\n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        score_avg = running_mean(idx_epi, score_hist)\n",
    "        mean_rewards.append(score_avg)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        # if (idx_epi+1) % 100 == 0:\n",
    "        #     print(\" \")\n",
    "        #     sleep(0.1)\n",
    "\n",
    "        # Early stop\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    return score_hist, mean_rewards\n",
    "\n",
    "#%% Test Lunar Lander\n",
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state = env.reset()\n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "\"\"\"\n",
    "def plotScore(scores, mean_rewards):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.ylabel(\"Rewards\",fontsize=12)\n",
    "    plt.xlabel(\"Episodes\",fontsize=12)\n",
    "    plt.plot(scores, \n",
    "             color='gray', \n",
    "             linewidth=1)\n",
    "    plt.scatter(np.arange(len(scores)),\n",
    "                scores,\n",
    "                color='green',\n",
    "                linewidth=0.3,\n",
    "                label = 'Episode Rewards')\n",
    "    plt.plot(mean_rewards,\n",
    "             color='blue',\n",
    "             linewidth=3,\n",
    "             label = 'Running Average Score of DQN Policy')\n",
    "\n",
    "    plt.title('Rewards by Episode For DQN in Lunar Lander')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 1500\n",
    "TARGET_SCORE = 250.     # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file\n",
    "\n",
    "\n",
    "#env = gym.make('LunarLander-v2')\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    "    )\n",
    "score_hist, mean_rewards = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=SAVE_CHKPT)\n",
    "#plotScore(score_hist, mean_rewards)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
